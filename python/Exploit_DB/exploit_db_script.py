import os
import re
import requests
from validate_filename import validate
from bs4 import BeautifulSoup as bs


headers = {"User-Agent": "Mozilla/5.0 (Windows NT 6.3; Win64; x64) \
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"}

def main():
	try:
		print(
			"""
		##############>>>>>>>>>>^^<<<<<<<<<<##############
		#      Exploit DB Security archive Script        #
		#                 By Ethical                     #
		#                June 2nd 2017                   #
		#    This Script Connect to Exploit-db.com and   #
		#    Search the Security archive for new Papers  #
		#   Download and archive them if non-existent on #
		#                  the System                    #
		##############>>>>>>>>>>^^<<<<<<<<<<##############
			   """)
		cre_links_file()
	except KeyboardInterrupt:
		print("Going to shutdown goodbye...")
	except Exception:
		print("Shutting down...")
		exit(0)


# this function creates the links file if non-existent
def cre_links_file():
	try:
		if not os.path.isfile(os.getcwd()+"/links.txt"):
			open("links.txt", "w")
			print "links file created successfully!"
		conn_exploitdb()
	except Exception, e:
		error(e)


# Connect to exploit-db Website...
def conn_exploitdb():
	while(True):
		try:
			ask = raw_input("""
1. Use this if you want to specify a new address
[this is in case if site changes...] otherwise,
2. Use this for default [https://www.exploit-db.com/papers/?order_by=date_published&order=desc&pg=01]
""")
			if ask == "1":
				address = raw_input("""
Please suppply a Valid address in this format
[Use this for default [https://www.exploit-db.com/papers/?order_by=date_published&order=desc&pg=01] :\n""")
				url_val = int(address[-2:])
			elif ask == "2":
				address = "https://www.exploit-db.com/papers/?order_by=date_published&order=desc&pg=01"
				url_val = int(address[-2:])
			else:
				raise Warning
			connect(address, url_val)
			break
		except Warning:
			print("Incorrect Input...")
		except Exception,e:
			error(e)
			break


def connect(address, url_val):
	try:
		with requests.Session() as s:
				page = s.get(address, headers=headers)
				content = page.content
				get_links(content, url_val)
	except Exception,e:
		error(e)



#This function retrieve links from the links.txt file
def get_links(content, url_val):
	try:
		links = []
		with open(os.getcwd()+"/links.txt", "r") as f:
			for lnk in f.readlines():
				links.append(lnk.strip("\n"))
		extract_links(links, content, url_val)
	except Exception,e:
		error(e)



# the function extract the links from the page parsed....
def extract_links(links, content, url_val):
	try:
		content_soup = bs(content, "html.parser")
		clean_content = content_soup.prettify("utf-8")
		clean_content_soup = bs(clean_content, "html.parser")
		td_find = clean_content_soup.find_all("td", attrs={"class":"description"})
		for found in td_find:
			try:
				found_soup = bs(str(found), "html.parser")
				find_link = found_soup.find("a", attrs={"href":True})
				link = find_link.get("href").encode("utf8")
				file_name = find_link.string.strip("\n").strip(" ").strip("\n")
				file_name = validate(file_name.encode("utf8"))
				if "/" in file_name:
					file_name = file_name.replace("/","_")
				elif "\\" in file_name:
					file_name = file_name.replace("\\","_")
				if link not in links and found is td_find[-1]:
					download_file(link, file_name, url_val)
				elif link not in links:
					download_file(link, file_name)
			except UnicodeError:
				raise
		if td_find:
			url_val = int(url_val)+1
			address = "https://www.exploit-db.com/papers/?order_by=date_published&order=desc&pg="+str(url_val)
			print("changing to page "+str(url_val))
			connect(address, str(url_val))
	except Exception,e:
		error(e)


#This function reads the link file and checks if the
#parsed link doesn't exist and download it...
#links ==> links retrieve 4rm txt file: link ==> link retrived 4rm site
def download_file(link, file_name, url_val=0):
	try:
		with requests.Session() as s:
			file = s.get(link, headers=headers)
		if not os.path.exists(os.getcwd()+"/Papers"):
			os.mkdir(os.getcwd()+"/Papers")
		if link[-4:] == ".pdf":
			ext = ".pdf"
		elif link[-4:] == ".txt":
			ext = ".txt"
		else:
			ext = ".html"
		if ext == ".txt":
			with open(os.getcwd()+"/Papers/"+file_name+ext, "w") as f:
				f.write(file.content)
		else:
			with open(os.getcwd()+"/Papers/"+file_name+ext, "wb") as f:
				f.write(file.content)
		with open(os.getcwd()+"/links.txt", "a") as a:
			a.write(link+"\n")
		print("Retrieved "+ file_name)
		if url_val != 0:
			recurse(url_val)
	except Exception,e:
		error(e)


#this function recurse through pages until there is no new link
def recurse(url_val):
	try:
		new_link = "https://www.exploit-db.com/papers/?order_by=date_published&order=desc&pg="+str(url_val+1)
		with requests.Session() as s:
			page = s.get(new_link, headers=headers)
		content = page.content
		url_val += 1
		print("changing to page "+str(url_val))
		get_links(content,url_val)
	except Exception,e:
		error(e)



#default error function
def error(e):
	print("Error: "+str(e))
	print("Initiating Forceful shutdown...")

if __name__ == "__main__":
	main()
